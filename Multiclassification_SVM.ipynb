{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn import svm\n",
    "import sklearn.model_selection as model_selection\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_mat(df):\n",
    "    X_mat = []\n",
    "    for i in df.index:\n",
    "        rowData = df.loc[i,:].values[0:10].tolist()\n",
    "        X_mat.append(rowData)\n",
    "    return np.array(X_mat)\n",
    "\n",
    "df = pd.read_csv('processed_data.csv', names = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'y_i'])\n",
    "X = feature_mat(df)\n",
    "y = df['y_i']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.80, test_size=0.20)\n",
    "\n",
    "# df = pd.read_csv('processed_data.csv', names = ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'y_i'])\n",
    "# dumb_df = df[['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10']]\n",
    "# DAYs_df = pd.DataFrame()\n",
    "# DAYs_df['TMAX'] = dumb_df.max(axis = 1)\n",
    "# DAYs_df['TMIN'] = dumb_df.min(axis = 1)\n",
    "# DAYs_df['TMEAN'] = dumb_df.mean(axis = 1)\n",
    "# DAYs_df['TVAR'] = dumb_df.var(axis = 1)\n",
    "\n",
    "# # Raw Feature & Labels\n",
    "# X = DAYs_df.values\n",
    "# y = df['y_i']\n",
    "\n",
    "# # Built-in Random SPLIT\n",
    "# X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.80, test_size=0.20)\n",
    "\n",
    "# Oversampling imbalanced minority class\n",
    "# oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
    "# X_over, y_over = oversample.fit_resample(X, y)\n",
    "# X_over, y_over = BorderlineSMOTE().fit_resample(X, y)\n",
    "X_over, y_over = SMOTE().fit_resample(X_train, y_train)\n",
    "X_over_test, y_over_test = SMOTE().fit_resample(X_test, y_test)\n",
    "\n",
    "# For plotting decision boundary\n",
    "# X_1_plt = df['D1']\n",
    "# X_2_plt = df['D2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_0 = np.size(df.index[df['y_i'] == 0].tolist())\n",
    "n_class_1 = np.size(df.index[df['y_i'] == 1].tolist())\n",
    "print(n_class_0)\n",
    "print(n_class_1)\n",
    "print(n_class_1*11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling train-data\n",
    "import random \n",
    "\n",
    "def Oversampling(X_train, y_train):\n",
    "    n_class_0 = y_train.to_list().count(0)\n",
    "    N = len(y_train.to_list())\n",
    "    n_classes = 11\n",
    "    X_temp = []\n",
    "    y_temp = []\n",
    "\n",
    "    for k in range(n_classes):\n",
    "        n_new_example = n_class_0-y_train.to_list().count(k+1)\n",
    "        index_arr = np.linspace(0,N-1,N, dtype=int)\n",
    "        new_example_index_list = np.random.choice(index_arr, size=n_new_example, replace=True)\n",
    "        \n",
    "        for i in new_example_index_list:\n",
    "            y_temp.append(y_train.to_list()[i])\n",
    "            X_temp.append(X_train[i])\n",
    "\n",
    "    X_over = np.concatenate((np.array(X_temp), X_train))\n",
    "    y_over = pd.Series(y_temp+y_train.to_list())\n",
    "    \n",
    "    return X_over, y_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_over, y_over = Oversampling(X_train, y_train)\n",
    "print(np.size(X_over), np.size(y_over), np.size(X_over_test), np.size(y_over_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model fitting and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有个大问题，我好想不应该用 SVC，得用 OneVsOneClassifier。。。。。因为三种跑法得到的 support vectors 数量一样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC Default use OVO method for classification -- Reasons for choosing OVO is explained in essay\n",
    "time_cost = []\n",
    "\n",
    "# fit the model with no wieghts/penalizations\n",
    "start = time.time()\n",
    "clf_no_weights = svm.SVC(kernel='rbf', gamma='auto', class_weight=None).fit(X_train, y_train)\n",
    "end = time.time()\n",
    "time_cost.append(round(end-start,4))\n",
    "\n",
    "# fit the model with proportional weights for minority class\n",
    "start = time.time()\n",
    "clf_weights = svm.SVC(kernel='rbf', gamma='auto', class_weight='balanced').fit(X_train, y_train)\n",
    "                                                # Set the parameter C of class i to class_weight[i]*C for SVC. \n",
    "                                                # The “balanced” mode uses the values of y to automatically adjust weights \n",
    "                                                # inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y))\n",
    "end = time.time()\n",
    "time_cost.append(round(end-start,4))\n",
    "\n",
    "\n",
    "# fit the model with oversampled training data\n",
    "start = time.time()\n",
    "clf_oversample = svm.SVC(kernel='rbf', gamma='auto', class_weight=None).fit(X_over, y_over)\n",
    "                                                # oversample the data for training have equivalent effect as changing \n",
    "                                                # class_weight proportionally \n",
    "end = time.time()\n",
    "time_cost.append(round(end-start,4))\n",
    "\n",
    "\n",
    "# Present Runtime of each\n",
    "print(time_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# print(scaler.fit(X_train))\n",
    "\n",
    "# # The standardization only applies for OnevsOne otherwise will cause failure of convergence\n",
    "# X_train_onevsone = scaler.transform(X_train)\n",
    "# X_train_test = scaler.transform(X_test)\n",
    "# print(X_train_onevsone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = OneVsOneClassifier(LinearSVC(random_state=0, max_iter = 100000)).fit(X_train_onevsone, y_train)\n",
    "# clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_vs_one_predict = clf.predict(X_train_test)\n",
    "# print(metrics.classification_report(y_test, one_vs_one_predict, zero_division=1, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No actions\n",
    "clf_SVM_pred = clf_no_weights.predict(X_test)\n",
    "# Use different misclassification penalties per class (C_i)\n",
    "wclf_SVM_pred = clf_weights.predict(X_test)\n",
    "# Oversampling training data with imbalance package\n",
    "oclf_SVM_pred = clf_oversample.predict(X_test)\n",
    "\n",
    "print(\"\\n____________________Unweighted multi-SVM_______________________\\n\")\n",
    "print(metrics.classification_report(y_test, clf_SVM_pred, zero_division=1, digits=6))\n",
    "print(\"\\n______________Proportionally Weighted multi-SVM________________\\n\")\n",
    "print(metrics.classification_report(y_test, wclf_SVM_pred, digits=6))\n",
    "print(\"\\n______________Oversample train-data multi-SVM__________________\\n\")\n",
    "print(metrics.classification_report(y_test, oclf_SVM_pred, zero_division=1, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nclf\")\n",
    "print(\"SVM Accuracy Score:\\t\", metrics.accuracy_score(clf_SVM_pred, y_test)*100)\n",
    "print(\"SVM f1 score:\\t\\t\", metrics.f1_score(clf_SVM_pred, y_test, average=\"macro\"))\n",
    "print(\"SVM precision score:\\t\", metrics.precision_score(clf_SVM_pred, y_test, average=\"macro\"))\n",
    "print(\"SVM recall score:\\t\", metrics.recall_score(clf_SVM_pred, y_test, average=\"macro\", zero_division=1))\n",
    "\n",
    "print(\"\\nwclf\")\n",
    "print(\"SVM Accuracy Score:\\t\", metrics.accuracy_score(wclf_SVM_pred, y_test)*100)\n",
    "print(\"SVM f1 score:\\t\\t\", metrics.f1_score(wclf_SVM_pred, y_test, average=\"macro\"))\n",
    "print(\"SVM precision score:\\t\", metrics.precision_score(wclf_SVM_pred, y_test, average=\"macro\"))\n",
    "print(\"SVM recall score:\\t\", metrics.recall_score(wclf_SVM_pred, y_test, average=\"macro\", zero_division=1))\n",
    "\n",
    "print(\"\\noclf\")\n",
    "print(\"SVM Accuracy Score:\\t\", metrics.accuracy_score(oclf_SVM_pred, y_test)*100)\n",
    "print(\"SVM f1 score:\\t\\t\", metrics.f1_score(oclf_SVM_pred, y_test, average=\"macro\"))\n",
    "print(\"SVM precision score:\\t\", metrics.precision_score(oclf_SVM_pred, y_test, average=\"macro\"))\n",
    "print(\"SVM recall score:\\t\", metrics.recall_score(oclf_SVM_pred, y_test, average=\"macro\", zero_division=1))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print(\"\\onevsone\")\n",
    "# print(\"SVM Accuracy Score:\\t\", metrics.accuracy_score(one_vs_one_predict, y_test)*100)\n",
    "# print(\"SVM f1 score:\\t\\t\", metrics.f1_score(one_vs_one_predict, y_test, average=\"macro\"))\n",
    "# print(\"SVM precision score:\\t\", metrics.precision_score(one_vs_one_predict, y_test, average=\"macro\"))\n",
    "# print(\"SVM recall score:\\t\", metrics.recall_score(one_vs_one_predict, y_test, average=\"macro\", zero_division=1))\n",
    "# print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVMs = [clf_SVM_pred, wclf_SVM_pred, oclf_SVM_pred]\n",
    "# dic = {}\n",
    "# dic.keys() = [\"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"]\n",
    "# for i in SVMs:\n",
    "#    dic[keys[0]] = metrics.accuracy_score(SVMs[i], y_test)*100\n",
    "#    dic[keys[1]] = metrics.f1_score(SVMs[i], y_test, average=\"macro\")\n",
    "#    dic[keys[2]] = metrics.precision_score(SVMs[i], y_test, average=\"macro\")\n",
    "#    dic[keys[3]] = metrics.recall_score(SVMs[i], y_test, average=\"macro\", zero_division=1)\n",
    "# print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_no_weights\n",
    "C = metrics.confusion_matrix(y_test, clf_SVM_pred)\n",
    "sns.heatmap(pd.DataFrame(C), annot=True, vmin=-1, vmax=30, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_weights\n",
    "C_weight = metrics.confusion_matrix(y_test, wclf_SVM_pred)\n",
    "sns.heatmap(pd.DataFrame(C_weight), annot=True, vmin=-1, vmax=30, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_oversample\n",
    "C_over = metrics.confusion_matrix(y_test, oclf_SVM_pred)\n",
    "sns.heatmap(pd.DataFrame(C_over),annot=True, vmin=-1, vmax=30, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PR-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.multiclass import OneVsOneClassifier\n",
    "# from sklearn.svm import LinearSVC\n",
    "# clf = OneVsOneClassifier(LinearSVC(random_state=0, max_iter=1e15)).fit(X_train, y_train)\n",
    "# clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # precision recall curve\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# precision = dict()\n",
    "# recall = dict()\n",
    "# n_classes = 11\n",
    "# for i in range(n_classes):\n",
    "#     precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "#                                                         y_score[:, i])\n",
    "#     plt.plot(recall[i], precision[i], lw=2, label='class {}'.format(i))\n",
    "    \n",
    "# plt.xlabel(\"recall\")\n",
    "# plt.ylabel(\"precision\")\n",
    "# plt.legend(loc=\"best\")\n",
    "# plt.title(\"precision vs. recall curve\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = clf_weights.decision_function(X_test)\n",
    "print(y_score[20:30])\n",
    "print(y_test[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_test_lst = y_test.tolist()\n",
    "print(y_test_lst[0:10])\n",
    "\n",
    "new_y_test = []\n",
    "for i in range(len(y_test_lst)):\n",
    "    tmp = [0 for i in range(11)]\n",
    "    num = y_test_lst[i]\n",
    "    tmp[num] = 1\n",
    "    new_y_test.append(tmp)\n",
    "\n",
    "new_y_test = np.asarray(new_y_test, dtype=int)\n",
    "print(new_y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(11):\n",
    "#     print(y_test)\n",
    "#     print(len(y_score), len(y_score[0]))\n",
    "    precision[i], recall[i], _ = precision_recall_curve(new_y_test[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(new_y_test[:, i], y_score[:, i])\n",
    "    \n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(new_y_test.ravel(), y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(new_y_test, y_score, average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f} clf_weights'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = clf_no_weights.decision_function(X_test)\n",
    "print(y_score[20:30])\n",
    "print(y_test[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_test_lst = y_test.tolist()\n",
    "print(y_test_lst[0:10])\n",
    "\n",
    "new_y_test = []\n",
    "for i in range(len(y_test_lst)):\n",
    "    tmp = [0 for i in range(11)]\n",
    "    num = y_test_lst[i]\n",
    "    tmp[num] = 1\n",
    "    new_y_test.append(tmp)\n",
    "\n",
    "new_y_test = np.asarray(new_y_test, dtype=int)\n",
    "print(new_y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(11):\n",
    "#     print(y_test)\n",
    "#     print(len(y_score), len(y_score[0]))\n",
    "    precision[i], recall[i], _ = precision_recall_curve(new_y_test[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(new_y_test[:, i], y_score[:, i])\n",
    "    \n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(new_y_test.ravel(), y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(new_y_test, y_score, average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f} clf_no_weights'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = clf_oversample.decision_function(X_test)\n",
    "print(y_score[20:30])\n",
    "print(y_test[20:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_test_lst = y_test.tolist()\n",
    "print(y_test_lst[0:10])\n",
    "\n",
    "new_y_test = []\n",
    "for i in range(len(y_test_lst)):\n",
    "    tmp = [0 for i in range(11)]\n",
    "    num = y_test_lst[i]\n",
    "    tmp[num] = 1\n",
    "    new_y_test.append(tmp)\n",
    "\n",
    "new_y_test = np.asarray(new_y_test, dtype=int)\n",
    "print(new_y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(11):\n",
    "#     print(y_test)\n",
    "#     print(len(y_score), len(y_score[0]))\n",
    "    precision[i], recall[i], _ = precision_recall_curve(new_y_test[:, i], y_score[:, i])\n",
    "    average_precision[i] = average_precision_score(new_y_test[:, i], y_score[:, i])\n",
    "    \n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(new_y_test.ravel(), y_score.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(new_y_test, y_score, average=\"micro\")\n",
    "print('Average precision score, micro-averaged over all classes: {0:0.2f}'.format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.step(recall['micro'], precision['micro'], where='post')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(\n",
    "    'Average precision score, micro-averaged over all classes: AP={0:0.2f} clf_oversample'\n",
    "    .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves of Multi-class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\space$\n",
    "\n",
    "## Contour map of decision boundary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
